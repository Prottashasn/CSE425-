{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Data Preprocessing for VAE Clustering\n",
    "\n",
    "This notebook preprocesses audio data for enhanced VAE clustering with:\n",
    "- **Convolutional VAE**: Mel-spectrograms for CNN architecture\n",
    "- **Hybrid Features**: Audio features + Lyrics embeddings\n",
    "- **Multiple Clustering**: K-Means, Agglomerative Clustering, DBSCAN\n",
    "- **Evaluation Metrics**: Silhouette Score, Davies-Bouldin Index, Adjusted Rand Index\n",
    "\n",
    "## Output Files:\n",
    "- `mel_spectrograms.npy` - Mel-spectrograms for Conv-VAE (N, n_mels, time_frames)\n",
    "- `audio_features.npy` - Traditional audio features (for comparison)\n",
    "- `lyrics_embeddings.npy` - Lyrics embeddings (TF-IDF based)\n",
    "- `hybrid_features.npy` - Combined audio + lyrics embeddings\n",
    "- `labels.npy` - Genre labels\n",
    "- `genre_mapping.pkl` - Genre to label mapping\n",
    "- `preprocessing_info.pkl` - Parameters and metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages if needed (uncomment for Colab)\n",
    "# !pip install scipy scikit-learn numpy pandas -q\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "BANGLA_DATASET_DIR = r'E:\\425 Project\\Datasets\\Bangla_Datasets'  # Change for Colab: '/content/Bangla_Datasets'\n",
    "ENGLISH_DATASET_DIR = r'E:\\425 Project\\Datasets\\English_Datasets'  # Change for Colab: '/content/English_Datasets'\n",
    "METADATA_FILE = r'E:\\425 Project\\Datasets\\updated_metadata.csv'  # Change for Colab: '/content/updated_metadata.csv'\n",
    "\n",
    "# Audio processing parameters\n",
    "TARGET_SR = 22050  # Target sample rate\n",
    "DURATION = 3.0  # Duration in seconds (for 3-second clips)\n",
    "N_MELS = 128  # Number of mel filterbanks for spectrogram\n",
    "N_FFT = 2048  # FFT window size\n",
    "HOP_LENGTH = 512  # Hop length for STFT\n",
    "\n",
    "# Feature extraction parameters\n",
    "MAX_SAMPLES_PER_GENRE = None  # None = use all samples\n",
    "\n",
    "# Lyrics embedding parameters\n",
    "LYRICS_EMBEDDING_DIM = 256  # Dimension for lyrics embeddings\n",
    "\n",
    "print(\"‚úÖ Configuration set!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_file(file_path, target_sr=22050, duration=3.0):\n",
    "    \"\"\"Load and preprocess WAV file.\"\"\"\n",
    "    try:\n",
    "        sr, audio = wavfile.read(file_path)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if len(audio.shape) > 1:\n",
    "            audio = np.mean(audio, axis=1)\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        if audio.dtype == np.int16:\n",
    "            audio = audio.astype(np.float32) / 32768.0\n",
    "        elif audio.dtype == np.int32:\n",
    "            audio = audio.astype(np.float32) / 2147483648.0\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sr != target_sr:\n",
    "            num_samples = int(len(audio) * target_sr / sr)\n",
    "            audio = signal.resample(audio, num_samples)\n",
    "        \n",
    "        # Trim or pad to target duration\n",
    "        target_length = int(target_sr * duration)\n",
    "        if len(audio) > target_length:\n",
    "            audio = audio[:target_length]\n",
    "        else:\n",
    "            audio = np.pad(audio, (0, target_length - len(audio)), mode='constant')\n",
    "        \n",
    "        return audio, target_sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return np.zeros(int(target_sr * duration)), target_sr\n",
    "\n",
    "\n",
    "def compute_mel_spectrogram(audio, sr=22050, n_mels=128, n_fft=2048, hop_length=512):\n",
    "    \"\"\"Compute mel-spectrogram using scipy.\"\"\"\n",
    "    # Compute STFT\n",
    "    f, t, stft = signal.stft(audio, sr, nperseg=n_fft, noverlap=n_fft-hop_length, window='hann')\n",
    "    \n",
    "    # Magnitude spectrum\n",
    "    magnitude = np.abs(stft)\n",
    "    power = magnitude ** 2\n",
    "    \n",
    "    # Create mel filterbank\n",
    "    n_freq_bins = power.shape[0]\n",
    "    n_filters = n_mels\n",
    "    filter_bank = np.zeros((n_filters, n_freq_bins))\n",
    "    \n",
    "    # Mel scale conversion\n",
    "    mel_points = np.linspace(0, n_freq_bins, n_filters + 2)\n",
    "    \n",
    "    for i in range(n_filters):\n",
    "        start = int(mel_points[i])\n",
    "        center = int(mel_points[i + 1])\n",
    "        end = int(mel_points[i + 2])\n",
    "        \n",
    "        if start < center:\n",
    "            filter_bank[i, start:center] = np.linspace(0, 1, center - start)\n",
    "        if center < end:\n",
    "            filter_bank[i, center:end] = np.linspace(1, 0, end - center)\n",
    "    \n",
    "    # Apply mel filterbank\n",
    "    mel_power = np.dot(filter_bank, power)\n",
    "    \n",
    "    # Convert to log scale (dB)\n",
    "    mel_spec_db = 10 * np.log10(mel_power + 1e-10)\n",
    "    \n",
    "    return mel_spec_db\n",
    "\n",
    "\n",
    "def extract_audio_features(audio, sr=22050):\n",
    "    \"\"\"Extract traditional audio features.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Spectral features\n",
    "    f, t, stft = signal.stft(audio, sr, nperseg=2048, noverlap=1536)\n",
    "    magnitude = np.abs(stft)\n",
    "    \n",
    "    # Mel-spectrogram features\n",
    "    mel_spec = compute_mel_spectrogram(audio, sr, n_mels=64)\n",
    "    \n",
    "    # Statistical features from mel-spectrogram\n",
    "    features.extend(np.mean(mel_spec, axis=1))  # Mean per mel band\n",
    "    features.extend(np.std(mel_spec, axis=1))   # Std per mel band\n",
    "    \n",
    "    # Spectral features\n",
    "    features.append(np.mean(magnitude))\n",
    "    features.append(np.std(magnitude))\n",
    "    \n",
    "    # Spectral centroid approximation\n",
    "    features.append(np.mean(np.diff(magnitude, axis=0)))\n",
    "    \n",
    "    # Zero crossing rate\n",
    "    zcr = np.mean(np.abs(np.diff(np.sign(audio)))) / 2\n",
    "    features.append(zcr)\n",
    "    \n",
    "    # Energy\n",
    "    features.append(np.mean(audio ** 2))\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load Dataset and Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata (for Bangla dataset)\n",
    "print(\"Loading metadata...\")\n",
    "try:\n",
    "    metadata_df = pd.read_csv(METADATA_FILE)\n",
    "    print(f\"‚úÖ Metadata loaded: {len(metadata_df)} entries\")\n",
    "    print(f\"   Columns: {list(metadata_df.columns)}\")\n",
    "    \n",
    "    # Create ID to lyrics mapping (for Bangla dataset)\n",
    "    id_to_lyrics = dict(zip(metadata_df['ID'], metadata_df['lyrics'].fillna('')))\n",
    "    print(f\"‚úÖ Lyrics mapping created: {len(id_to_lyrics)} entries\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not load metadata file: {e}\")\n",
    "    id_to_lyrics = {}\n",
    "\n",
    "# Load audio files from both datasets\n",
    "audio_files = []\n",
    "file_ids = []\n",
    "labels = []\n",
    "dataset_types = []  # Track which dataset each file comes from\n",
    "genre_to_label = {}\n",
    "label_to_genre = {}\n",
    "\n",
    "# Process Bangla dataset\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading BANGLA dataset...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bangla_genres = [d for d in os.listdir(BANGLA_DATASET_DIR) if os.path.isdir(os.path.join(BANGLA_DATASET_DIR, d))]\n",
    "bangla_genres.sort()\n",
    "\n",
    "label_offset = 0\n",
    "for genre in bangla_genres:\n",
    "    genre_to_label[genre] = label_offset\n",
    "    label_to_genre[label_offset] = genre\n",
    "    label_offset += 1\n",
    "    \n",
    "    genre_path = os.path.join(BANGLA_DATASET_DIR, genre)\n",
    "    files = [f for f in os.listdir(genre_path) if f.endswith('.wav')]\n",
    "    files.sort()\n",
    "    \n",
    "    if MAX_SAMPLES_PER_GENRE:\n",
    "        files = files[:MAX_SAMPLES_PER_GENRE]\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(genre_path, file)\n",
    "        file_id = os.path.splitext(file)[0]  # Remove .wav extension\n",
    "        \n",
    "        audio_files.append(file_path)\n",
    "        file_ids.append(file_id)\n",
    "        labels.append(genre_to_label[genre])\n",
    "        dataset_types.append('bangla')\n",
    "    \n",
    "    print(f\"  {genre}: {len(files)} files\")\n",
    "\n",
    "# Process English dataset\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading ENGLISH dataset...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "english_genres = [d for d in os.listdir(ENGLISH_DATASET_DIR) if os.path.isdir(os.path.join(ENGLISH_DATASET_DIR, d))]\n",
    "english_genres.sort()\n",
    "\n",
    "# Remove jazz genre (no lyrics available)\n",
    "if 'jazz' in english_genres:\n",
    "    english_genres.remove('jazz')\n",
    "    print(\"‚ö†Ô∏è  Excluded 'jazz' genre (no lyrics available)\")\n",
    "\n",
    "for genre in english_genres:\n",
    "    genre_to_label[genre] = label_offset\n",
    "    label_to_genre[label_offset] = genre\n",
    "    label_offset += 1\n",
    "    \n",
    "    genre_path = os.path.join(ENGLISH_DATASET_DIR, genre)\n",
    "    files = [f for f in os.listdir(genre_path) if f.endswith('.wav')]\n",
    "    files.sort()\n",
    "    \n",
    "    if MAX_SAMPLES_PER_GENRE:\n",
    "        files = files[:MAX_SAMPLES_PER_GENRE]\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(genre_path, file)\n",
    "        file_id = os.path.splitext(file)[0]  # Remove .wav extension\n",
    "        \n",
    "        audio_files.append(file_path)\n",
    "        file_ids.append(file_id)\n",
    "        labels.append(genre_to_label[genre])\n",
    "        dataset_types.append('english')\n",
    "    \n",
    "    print(f\"  {genre}: {len(files)} files\")\n",
    "\n",
    "labels = np.array(labels)\n",
    "dataset_types = np.array(dataset_types)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ Total samples: {len(audio_files)}\")\n",
    "print(f\"   - Bangla: {np.sum(dataset_types == 'bangla')}\")\n",
    "print(f\"   - English: {np.sum(dataset_types == 'english')}\")\n",
    "print(f\"‚úÖ Number of genres: {len(genre_to_label)}\")\n",
    "print(f\"‚úÖ Genres: {', '.join(sorted(genre_to_label.keys()))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Extract Mel-Spectrograms (for Conv-VAE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting mel-spectrograms...\")\n",
    "mel_spectrograms = []\n",
    "\n",
    "for i, file_path in enumerate(audio_files):\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(audio_files)} files...\")\n",
    "    \n",
    "    audio, sr = load_wav_file(file_path, TARGET_SR, DURATION)\n",
    "    mel_spec = compute_mel_spectrogram(audio, sr, N_MELS, N_FFT, HOP_LENGTH)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    mel_spec = (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min() + 1e-8)\n",
    "    \n",
    "    mel_spectrograms.append(mel_spec)\n",
    "\n",
    "mel_spectrograms = np.array(mel_spectrograms)\n",
    "print(f\"\\n‚úÖ Mel-spectrograms extracted!\")\n",
    "print(f\"   Shape: {mel_spectrograms.shape} (samples, n_mels, time_frames)\")\n",
    "print(f\"   Min: {mel_spectrograms.min():.4f}, Max: {mel_spectrograms.max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Extract Audio Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting audio features...\")\n",
    "audio_features_list = []\n",
    "\n",
    "for i, file_path in enumerate(audio_files):\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(audio_files)} files...\")\n",
    "    \n",
    "    audio, sr = load_wav_file(file_path, TARGET_SR, DURATION)\n",
    "    features = extract_audio_features(audio, sr)\n",
    "    audio_features_list.append(features)\n",
    "\n",
    "audio_features = np.array(audio_features_list)\n",
    "print(f\"\\n‚úÖ Audio features extracted!\")\n",
    "print(f\"   Shape: {audio_features.shape} (samples, features)\")\n",
    "\n",
    "# Standardize audio features\n",
    "audio_scaler = StandardScaler()\n",
    "audio_features_scaled = audio_scaler.fit_transform(audio_features)\n",
    "print(f\"‚úÖ Audio features standardized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Extract Lyrics Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lyrics for each file (only available for Bangla dataset)\n",
    "print(\"Extracting lyrics for each file...\")\n",
    "lyrics_list = []\n",
    "\n",
    "for i, file_id in enumerate(file_ids):\n",
    "    if dataset_types[i] == 'bangla':\n",
    "        lyrics = id_to_lyrics.get(file_id, '')\n",
    "    else:\n",
    "        lyrics = ''  # English dataset doesn't have lyrics metadata\n",
    "    lyrics_list.append(lyrics)\n",
    "\n",
    "print(f\"‚úÖ Lyrics extracted for {len(lyrics_list)} files\")\n",
    "files_with_lyrics = sum(1 for l in lyrics_list if l and len(l.strip()) > 0)\n",
    "print(f\"   Files with lyrics: {files_with_lyrics} (Bangla only)\")\n",
    "\n",
    "# Create TF-IDF embeddings\n",
    "print(\"\\nCreating TF-IDF embeddings...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=LYRICS_EMBEDDING_DIM,\n",
    "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "    min_df=2,  # Minimum document frequency\n",
    "    stop_words=None  # Keep all words (multilingual)\n",
    ")\n",
    "\n",
    "lyrics_embeddings = vectorizer.fit_transform(lyrics_list).toarray()\n",
    "print(f\"‚úÖ Lyrics embeddings created!\")\n",
    "print(f\"   Shape: {lyrics_embeddings.shape} (samples, embedding_dim)\")\n",
    "print(f\"   Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "print(f\"   Note: English files will have zero embeddings (no lyrics metadata)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Create Hybrid Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine audio features and lyrics embeddings\n",
    "print(\"Creating hybrid features (audio + lyrics)...\")\n",
    "\n",
    "# Standardize lyrics embeddings\n",
    "lyrics_scaler = StandardScaler()\n",
    "lyrics_embeddings_scaled = lyrics_scaler.fit_transform(lyrics_embeddings)\n",
    "\n",
    "# Concatenate audio features and lyrics embeddings\n",
    "hybrid_features = np.hstack([audio_features_scaled, lyrics_embeddings_scaled])\n",
    "\n",
    "# Standardize hybrid features\n",
    "hybrid_scaler = StandardScaler()\n",
    "hybrid_features_scaled = hybrid_scaler.fit_transform(hybrid_features)\n",
    "\n",
    "print(f\"‚úÖ Hybrid features created!\")\n",
    "print(f\"   Audio features dimension: {audio_features_scaled.shape[1]}\")\n",
    "print(f\"   Lyrics embeddings dimension: {lyrics_embeddings_scaled.shape[1]}\")\n",
    "print(f\"   Hybrid features dimension: {hybrid_features_scaled.shape[1]}\")\n",
    "print(f\"   Shape: {hybrid_features_scaled.shape} (samples, features)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Save Preprocessed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all preprocessed data\n",
    "print(\"Saving preprocessed data...\")\n",
    "\n",
    "# Save mel-spectrograms\n",
    "np.save('mel_spectrograms.npy', mel_spectrograms)\n",
    "print(\"‚úÖ Saved: mel_spectrograms.npy\")\n",
    "\n",
    "# Save audio features\n",
    "np.save('audio_features.npy', audio_features_scaled)\n",
    "print(\"‚úÖ Saved: audio_features.npy\")\n",
    "\n",
    "# Save lyrics embeddings\n",
    "np.save('lyrics_embeddings.npy', lyrics_embeddings_scaled)\n",
    "print(\"‚úÖ Saved: lyrics_embeddings.npy\")\n",
    "\n",
    "# Save hybrid features\n",
    "np.save('hybrid_features.npy', hybrid_features_scaled)\n",
    "print(\"‚úÖ Saved: hybrid_features.npy\")\n",
    "\n",
    "# Save labels\n",
    "np.save('labels.npy', labels)\n",
    "print(\"‚úÖ Saved: labels.npy\")\n",
    "\n",
    "# Save genre mapping\n",
    "with open('genre_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump({'genre_to_label': genre_to_label, 'label_to_genre': label_to_genre}, f)\n",
    "print(\"‚úÖ Saved: genre_mapping.pkl\")\n",
    "\n",
    "# Save scalers and vectorizer\n",
    "preprocessing_info = {\n",
    "    'audio_scaler': audio_scaler,\n",
    "    'lyrics_scaler': lyrics_scaler,\n",
    "    'hybrid_scaler': hybrid_scaler,\n",
    "    'lyrics_vectorizer': vectorizer,\n",
    "    'n_mels': N_MELS,\n",
    "    'n_fft': N_FFT,\n",
    "    'hop_length': HOP_LENGTH,\n",
    "    'target_sr': TARGET_SR,\n",
    "    'duration': DURATION,\n",
    "    'audio_feature_dim': audio_features_scaled.shape[1],\n",
    "    'lyrics_embedding_dim': lyrics_embeddings_scaled.shape[1],\n",
    "    'hybrid_feature_dim': hybrid_features_scaled.shape[1]\n",
    "}\n",
    "\n",
    "with open('preprocessing_info.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessing_info, f)\n",
    "print(\"‚úÖ Saved: preprocessing_info.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìÅ ALL PREPROCESSED DATA SAVED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  1. mel_spectrograms.npy - For Convolutional VAE\")\n",
    "print(\"  2. audio_features.npy - Traditional audio features\")\n",
    "print(\"  3. lyrics_embeddings.npy - Lyrics TF-IDF embeddings\")\n",
    "print(\"  4. hybrid_features.npy - Audio + Lyrics combined\")\n",
    "print(\"  5. labels.npy - Genre labels\")\n",
    "print(\"  6. genre_mapping.pkl - Genre mappings\")\n",
    "print(\"  7. preprocessing_info.pkl - Preprocessing metadata\")\n",
    "print(\"\\n‚úÖ Ready for enhanced VAE clustering!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
